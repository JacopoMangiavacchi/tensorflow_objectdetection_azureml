{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import string\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required data folder structure\n",
    "    proj_root_anyname\n",
    "      data\n",
    "        anyname1\n",
    "          Annotations\n",
    "          JPEGImages\n",
    "          pascal_label_map.pbtxt\n",
    "        anyname2\n",
    "          Annotations\n",
    "          JPEGImages\n",
    "          pascal_label_map.pbtxt\n",
    "        anynameN\n",
    "        ...\n",
    "    models\n",
    "      faster_rcnn_anyname\n",
    "        -- must have model.ckpt.data*\n",
    "        pipeline.config\n",
    "    tfrecords -- output folder\n",
    "      pascal_label_map.pbtxt \n",
    "      pipeline.config -- with path, num_classes, num_steps correctly set\n",
    "      test.record \n",
    "      train.record\n",
    "      val.record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_root='pets'\n",
    "base_model = 'faster_rcnn_resnet101_coco_2018_01_28'\n",
    "force_regenerate_tfrecords = False\n",
    "training_steps = 10\n",
    "support_gpu = True\n",
    "classname_in_filename = True #pet data doesn't have class in annotation xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\")\n",
    "proj_datastore = os.getenv(\"PROJ_DATASTORE\", default = None)\n",
    "logs_datastore = os.getenv(\"LOGS_DATASTORE\", default = 'logsds')\n",
    "compute_cpu = os.getenv(\"AML_COMPUTE_CPU\", default = 'amlcpu')\n",
    "compute_gpu = os.getenv(\"AML_COMPUTE_GPU\", default = 'amlnv6')\n",
    "\n",
    "docker_registry_address = os.getenv(\"ACR_ID\")\n",
    "docker_registry_username = os.getenv(\"ACR_USERNAME\")\n",
    "docker_registry_password = os.getenv(\"ACR_PASSWORD\")\n",
    "training_docker_image_short_name = os.getenv(\"TRAINING_DOCKER_SHORT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SUBDIR='data'\n",
    "TFRECORDS_SUBDIR='tfrecords'\n",
    "MODELS_SUBDIR='models'\n",
    "SCRIPT_FOLDER = './scripts'\n",
    "SCRIPT_FILE = 'train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Azure ML environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Run, Datastore\n",
    "from azureml.core.runconfig import ContainerRegistry\n",
    "\n",
    "ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one time set up for a datastore that contains the project root\n",
    "#ds = Datastore.register_azure_blob_container(\n",
    "#        workspace=ws,\n",
    "#        datastore_name=proj_datastore,\n",
    "#        container_name='container_name_for_proj_root',\n",
    "#        account_name='storage_account',\n",
    "#        account_key='account_key',\n",
    "#        create_if_not_exists=True)\n",
    "# optionally set it to default datastore for the AML workspace\n",
    "#ws.set_default_datastore(proj_datastore)\n",
    "\n",
    "# one time set up for a datastore that will contain Tensorflow logs for Tensorboard\n",
    "#dslogs = Datastore.register_azure_blob_container(\n",
    "#        workspace=ws,\n",
    "#        datastore_name=logs_datastore,\n",
    "#        container_name='container_name_for_logs',\n",
    "#        account_name='same_account_as_the_default_datastore_for_azureml',\n",
    "#        account_key='account_key',\n",
    "#        create_if_not_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if proj_datastore is None:\n",
    "    ds = ws.get_default_datastore()\n",
    "else:\n",
    "    ds = Datastore.get(ws, datastore_name=proj_datastore)\n",
    "dslogs = Datastore.get(ws, datastore_name=logs_datastore)\n",
    "print(ds.container_name, dslogs.container_name)\n",
    "\n",
    "compute_name = compute_gpu if support_gpu else compute_cpu\n",
    "compute_target = ws.compute_targets[compute_name]\n",
    "model_name = proj_root if proj_root.isalnum() else ''.join(ch for ch in proj_root if ch.isalnum())\n",
    "experiment_name = model_name\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "print(\"datastore:{}, compute:{}\".format(ds.container_name, type(compute_target)))\n",
    "print(\"proj_root:{}, model_name:{}\".format(proj_root, model_name))\n",
    "\n",
    "image_registry_details = ContainerRegistry()\n",
    "image_registry_details.address = docker_registry_address\n",
    "image_registry_details.username = docker_registry_username\n",
    "image_registry_details.password = docker_registry_password\n",
    "training_docker_image = docker_registry_address + '/' + training_docker_image_short_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook specific settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "tensorboard_local_dir_prefix = '/mnt/pliu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with a Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration, DataReferenceConfiguration\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                                path_on_datastore=proj_root,\n",
    "#                                path_on_compute='/datastore', path_on_compute doesn't work with mount\n",
    "                                overwrite=True)\n",
    "drlogs = DataReferenceConfiguration(datastore_name=dslogs.name, \n",
    "                                path_on_datastore=proj_root,\n",
    "#                                path_on_compute='/datastore', path_on_compute doesn't work with mount\n",
    "                                overwrite=True)\n",
    "\n",
    "run_cfg = RunConfiguration()\n",
    "run_cfg.environment.docker.enabled = True\n",
    "run_cfg.environment.docker.gpu_support = support_gpu\n",
    "run_cfg.environment.docker.base_image = training_docker_image # docker image fullname\n",
    "run_cfg.environment.docker.base_image_registry = image_registry_details\n",
    "run_cfg.data_references = {ds.name: dr, dslogs.name: drlogs} #tell the system to mount, later ds.mount() means mount from this path not root\n",
    "#extra arguments to docker run\n",
    "#run_amlcompute.environment.docker.arguments = <xref:azureml.core.runconfig.list>\n",
    "run_cfg.environment.python.user_managed_dependencies = True\n",
    "#run_cfg.auto_prepare_environment = False\n",
    "\n",
    "run_cfg.target = compute_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDT = datetime.datetime.now()\n",
    "currentDTstr = currentDT.strftime(\"%Y%m%d_%H%M\")\n",
    "print('logs will be in {}'.format(currentDTstr))\n",
    "\n",
    "base_mount = ds.as_mount() #this corresponds to run_cfg.data_referencese, so here it starts from proj_root rather than ds root path\n",
    "data_dir = os.path.join(str(base_mount), DATA_SUBDIR)\n",
    "tfrecords_dir = os.path.join(str(base_mount), TFRECORDS_SUBDIR)\n",
    "base_model_dir = os.path.join(str(base_mount), MODELS_SUBDIR, base_model)\n",
    "logs_mount = dslogs.as_mount()\n",
    "logs_dir = os.path.join(str(logs_mount), currentDTstr)\n",
    "\n",
    "script_params = [\n",
    "    '--data_dir', data_dir,\n",
    "    '--base_model_dir', base_model_dir, \n",
    "    '--tfrecords_dir', tfrecords_dir,\n",
    "    '--force_regenerate_tfrecords', force_regenerate_tfrecords,\n",
    "    '--num_steps', training_steps,\n",
    "    '--log_dir', logs_dir,\n",
    "    '--classname_in_filename', classname_in_filename\n",
    "]\n",
    "\n",
    "src = ScriptRunConfig(source_directory = SCRIPT_FOLDER, http://pliudsvm.westcentralus.cloudapp.azure.com:8888/notebooks/src/tensorflow_objectdetection_azureml/aml_train/aml-train.ipynb#\n",
    "                      script = SCRIPT_FILE, \n",
    "                      run_config = run_cfg,\n",
    "                      arguments=script_params)\n",
    "\n",
    "run = exp.submit(src)\n",
    "print('run details {}'.format(run.get_details))\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export logs locally for Tensorboard\n",
    "-  the below method works if logs are not large\n",
    "-  if logs are large, AML will timeout, so export logs to another data source, and simply download from Azure blob storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "local_tensorboard_logdir = os.path.join(tensorboard_local_dir_prefix, experiment_name)\n",
    "os.makedirs(local_tensorboard_logdir, exist_ok=True)\n",
    "\n",
    "# if a previous run rather than the one done in this session\n",
    "#run_id = 'previous_run_id'\n",
    "#exp = Experiment(workspace=ws, name=experiment_name)\n",
    "#run = Run(experiment = exp, run_id = run_id)\n",
    "tb = Tensorboard([run], local_root=local_tensorboard_logdir)\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "tb.start() #start may fail, but you can manually run tensorboard --logdir=local_tensorboard_logdir\n",
    "#tb.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or train with an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "currentDTstr = currentDT.strftime(\"%Y%m%d_%H%M\")\n",
    "print('logs will be in {}'.format(currentDTstr))\n",
    "\n",
    "# notice the base starts with proj_root, different from ScriptRun\n",
    "base_mount = ds.path(proj_root).as_mount()\n",
    "data_dir = os.path.join(str(base_mount), DATA_SUBDIR)\n",
    "tfrecords_dir = os.path.join(str(base_mount), TFRECORDS_SUBDIR)\n",
    "base_model_dir = os.path.join(str(base_mount), MODELS_SUBDIR, base_model)\n",
    "logs_mount = dslogs.path(proj_root).as_mount()\n",
    "logs_dir = os.path.join(str(logs_mount), currentDTstr)\n",
    "\n",
    "# notice the different format for parameters from ScriptRun\n",
    "script_params = {\n",
    "    '--data_dir': data_dir,\n",
    "    '--base_model_dir': base_model_dir, \n",
    "    '--tfrecords_dir': tfrecords_dir,\n",
    "    '--force_regenerate_tfrecords': force_regenerate_tfrecords,\n",
    "    '--num_steps': training_steps,\n",
    "    '--log_dir': logs_dir,\n",
    "    '--classname_in_filename': classname_in_filename\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=SCRIPT_FOLDER,\n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script=SCRIPT_FILE,\n",
    "                    use_docker=True,\n",
    "                    use_gpu=support_gpu,\n",
    "                    image_registry_details=image_registry_details,\n",
    "                    user_managed=True,\n",
    "                    custom_docker_image=training_docker_image_short_name, #notice this is short name, different from ScriptRun\n",
    "                    inputs=[base_mount, logs_mount]) #tell the system to mount, or if the script params contain ds.mount(), it will mount without this\n",
    "\n",
    "run = exp.submit(est)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or train with a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "currentDTstr = currentDT.strftime(\"%Y%m%d_%H%M\")\n",
    "print('logs will be in {}'.format(currentDTstr))\n",
    "\n",
    "#even though runcfg is used in pipeline, datareference in runcfg is ignored, so it's similar to Estimator\n",
    "# base starts with proj_root\n",
    "# you have to specify inputs/outputs to get mounted\n",
    "base_mount = ds.path(proj_root).as_mount() \n",
    "data_dir = os.path.join(str(base_mount), DATA_SUBDIR)\n",
    "tfrecords_dir = os.path.join(str(base_mount), TFRECORDS_SUBDIR)\n",
    "base_model_dir = os.path.join(str(base_mount), MODELS_SUBDIR, base_model)\n",
    "logs_mount = dslogs.path(proj_root).as_mount()\n",
    "logs_dir = os.path.join(str(logs_mount), currentDTstr)\n",
    "\n",
    "script_params = [\n",
    "    '--data_dir', data_dir,\n",
    "    '--base_model_dir', base_model_dir, \n",
    "    '--tfrecords_dir', tfrecords_dir,\n",
    "    '--force_regenerate_tfrecords', force_regenerate_tfrecords,\n",
    "    '--num_steps', training_steps,\n",
    "    '--log_dir', logs_dir,\n",
    "    '--classname_in_filename', classname_in_filename\n",
    "]\n",
    "\n",
    "trainStep = PythonScriptStep(\n",
    "    source_directory=SCRIPT_FOLDER,\n",
    "    script_name=SCRIPT_FILE,\n",
    "    name=\"train_step\",\n",
    "    arguments=script_params, \n",
    "    inputs=[base_mount, logs_mount], \n",
    "    #outputs=[output_tfrecords],#this is used for intermediate data, can be accessed by following steps, but not blob\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_cfg,\n",
    "    allow_reuse=False, #if true, reuse previous results if settings/inputs are same\n",
    "    version='0.1' #version tag to denote a change in functionality of this step\n",
    "    #params=dict of name/value pairs, env variables as \"AML_PARAMETER_\"\n",
    ")\n",
    "\n",
    "steps = [trainStep]\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline.validate()\n",
    "run = Experiment(ws, experiment_name).submit(pipeline)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_id = 'existing_run_id_rather_than_the_one_just_trained'\n",
    "#exp = Experiment(workspace=ws, name=experiment_name)\n",
    "#run = Run(exp, run_id)\n",
    "model = run.register_model(model_name=model_name, model_path='outputs/model/frozen_inference_graph.pb')\n",
    "print('registered model {}, version: {}'.format(model.name, model.version))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
